{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "version = list(map(int, torchtext.__version__.split('.')))\n",
    "if version[0] <= 0 and version[1] < 9:\n",
    "    from torchtext import data\n",
    "else:\n",
    "    from torchtext.legacy import data\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    '''\n",
    "    Data loader class to load text file using torchtext library.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, train_fn,\n",
    "        batch_size=64,\n",
    "        valid_ratio=.2,\n",
    "        device=-1,\n",
    "        max_vocab=999999,\n",
    "        min_freq=1,\n",
    "        use_eos=False,\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        '''\n",
    "        DataLoader initialization.\n",
    "        :param train_fn: Train-set filename\n",
    "        :param batch_size: Batchify data fot certain batch size.\n",
    "        :param device: Device-id to load data (-1 for CPU)\n",
    "        :param max_vocab: Maximum vocabulary size\n",
    "        :param min_freq: Minimum frequency for loaded word.\n",
    "        :param use_eos: If it is True, put <EOS> after every end of sentence.\n",
    "        :param shuffle: If it is True, random shuffle the input data.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Define field of the input file.\n",
    "        # The input file consists of two fields.\n",
    "        self.label = data.Field(\n",
    "            sequential=False,\n",
    "            use_vocab=True,\n",
    "            unk_token=None\n",
    "        )\n",
    "        self.text = data.Field(\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=False,\n",
    "            eos_token='<EOS>' if use_eos else None,\n",
    "        )\n",
    "\n",
    "        # Those defined two columns will be delimited by TAB.\n",
    "        # Thus, we use TabularDataset to load two columns in the input file.\n",
    "        # We would have two separate input file: train_fn, valid_fn\n",
    "        # Files consist of two columns: label field and text field.\n",
    "        train, valid = data.TabularDataset(\n",
    "            path=train_fn,\n",
    "            format='tsv', \n",
    "            fields=[\n",
    "                ('label', self.label),\n",
    "                ('text', self.text),\n",
    "            ],\n",
    "        ).split(split_ratio=(1 - valid_ratio))\n",
    "\n",
    "        # Those loaded dataset would be feeded into each iterator:\n",
    "        # train iterator and valid iterator.\n",
    "        # We sort input sentences by length, to group similar lengths.\n",
    "        self.train_loader, self.valid_loader = data.BucketIterator.splits(\n",
    "            (train, valid),\n",
    "            batch_size=batch_size,\n",
    "            device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "            shuffle=shuffle,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            sort_within_batch=True,\n",
    "        )\n",
    "\n",
    "        # At last, we make a vocabulary for label and text field.\n",
    "        # It is making mapping table between words and indice.\n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
